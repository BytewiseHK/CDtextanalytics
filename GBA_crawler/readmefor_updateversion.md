#How to run:
User Interaction:
Keywords: When prompted, enter keywords related to the news you're interested in. Example:
Enter keywords (comma-separated, use Chinese characters, example: æ”¿ç­–, è§„åˆ’, å»ºè®¾, äº¤é€š, å‘å±•, ä¼šè®®, é¡¹ç›®): 
If you don't enter any, default keywords will be used. 
Days Range: Input how many days back you want to check for articles:
Enter the number of days for article publication range (1-7): 
Cities Selection: Specify which cities you want to monitor from the list provided:
Enter cities to search (comma-separated, available options: Guangzhou, Shenzhen, Zhuhai, Dongguan, Zhongshan, Jiangmen, Zhaoqing, Huizhou, South China Net, Hong Kong, Macau):
Start Crawling: Press Enter to start the crawling process.
Interactive Commands:
stop: Stops the crawler.
status: Shows current status like processed pages and found articles.
articles: Lists or shows some found articles.
future: Displays articles with future events.
search: Searches through found articles for keywords.
keywords: Adds new keywords to the search criteria.
days: Changes the days range for article publication.
cities: Allows reselection of cities to monitor.
Monitor Progress: You'll see real-time updates on how many pages have been processed and articles found.

Output
Articles: Saved as JSON files in the specified output directory.
Log: Crawling operations are logged in crawl_log.jsonl.
Report: An HTML report summarizing the findings will be generated at the end in the output directory named report.html.

And you can stop the process by pressing Ctrl + C to end it earlierly.


# ğŸŒ‰ ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ–°é—»ç›‘æµ‹ç³»ç»Ÿ

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

æ™ºèƒ½ç›‘æµ‹å¤§æ¹¾åŒºå„åŸå¸‚æ–°é—»åŠ¨æ€ï¼Œè‡ªåŠ¨è¯†åˆ«æœªæ¥é‡è¦äº‹ä»¶å¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- å¤šæºæ–°é—»é‡‡é›†ï¼ˆæ”¯æŒå¹¿å·æ—¥æŠ¥ã€æ·±åœ³æ–°é—»ç½‘ç­‰ä¸»æµåª’ä½“ï¼‰
- æ™ºèƒ½å†…å®¹è¯†åˆ«ï¼ˆæ”¿ç­–/åŸºå»º/ç»æµç›¸å…³æŠ¥é“ï¼‰
- æœªæ¥äº‹ä»¶æ£€æµ‹ï¼ˆè‡ªåŠ¨è¯†åˆ«æ–°é—»ä¸­çš„æœªæ¥æ—¶é—´èŠ‚ç‚¹ï¼‰
- åŠ¨æ€å»é‡æœºåˆ¶ï¼ˆåŸºäºSimhashçš„å†…å®¹æŒ‡çº¹æŠ€æœ¯ï¼‰
- è‡ªåŠ¨ç”ŸæˆHTMLç®€æŠ¥ï¼ˆæ”¯æŒæ—¶é—´çº¿å¯è§†åŒ–ï¼‰

## âš™ï¸ ç³»ç»Ÿé…ç½®

```python
CONFIG = {
    "sources": {
        "å¹¿å·": "https://news.dayoo.com/guangzhou/",
        "æ·±åœ³": "https://www.sznews.com/news/",
        "ç æµ·": "https://news.hizh.cn/",
        'ä¸œè': 'https://news.sun0769.com/dg/headnews/',
        'ä¸­å±±': 'https://www.zsnews.cn/news/',
        'æ±Ÿé—¨': 'http://www.jmnews.com.cn/',
        'è‚‡åº†': 'http://www.xjrb.com/',
        'æƒ å·': 'http://www.huizhou.cn/',
        'å—æ–¹ç½‘': 'https://news.southcn.com/',
        'æ·±åœ³ä¼šå±•ä¸­å¿ƒ': 'https://www.szcec.com/Schedule/index.html%23yue9%EF%BC%8C',
        'é¦™æ¸¯æ—…æ¸¸å±€': 'https://www.discoverhongkong.com/hk-tc/what-s-new/events.html',
        'é¦™æ¸¯è´¸å‘å±€': 'https://event.hktdc.com/tc/',
        'æ”¿åºœç»Ÿè®¡å¤„': 'https://www.censtatd.gov.hk/sc/',
        'æ”¿åºœæ–°é—»ç½‘': 'https://www.info.gov.hk/gia/general/ctoday.htm',
        'é¦™æ¸¯æ—…æ¸¸ç½‘': 'https://partnernet.hktb.com/en/trade_support/trade_events/conventions_exhibitions/index.html?displayMode=&viewMode=calendar&isSearch=true&keyword=&area=0&location=&from=&to=&searchMonth=--+Please+Select+--&ddlDisplayMode_selectOneMenu=All',
        'æ¾³é—¨æ—…æ¸¸å±€': 'https://www.macaotourism.gov.mo/zh-hant/events/'
    },
    "keywords": ["æ”¿ç­–", "å³°ä¼š", "åŸºå»º"],
    "output_dir": "reports",
    "schedule": "daily 08:00"
}


â”œâ”€â”€ crawlers/            # çˆ¬è™«æ¨¡å—
â”œâ”€â”€ analysis/            # å†…å®¹åˆ†ææ¨¡å—
â”œâ”€â”€ utils/               # å·¥å…·å‡½æ•°
â”œâ”€â”€ config.py            # é…ç½®æ–‡ä»¶
â”œâ”€â”€ main.py              # ä¸»ç¨‹åº
â””â”€â”€ scheduler.py         # å®šæ—¶ä»»åŠ¡



# GBANewsMonitor

A Python script designed to monitor and collect news articles from specified Chinese cities with an emphasis on future-oriented content. This crawler uses asynchronous programming for efficient web scraping, filtering articles based on user-defined keywords and publication dates.

## Features

### User Interaction
- **Keyword Selection**: Users can input keywords in Chinese to filter articles. Default keywords include topics like policy, planning, construction, traffic, development, meetings, and projects.
- **Date Range**: Users define how many days back the articles should be searched from the current date.
- **City Selection**: Users can select one or multiple cities from a predefined list to focus the crawl.

### Crawling Capabilities
- **Asynchronous Crawling**: Utilizes `aiohttp` for non-blocking HTTP requests, enabling concurrent processing of web pages.
- **Depth Control**: Limits how deep into the website structure the crawler goes (`max_depth`).
- **Page Limit**: Stops after processing a maximum number of pages (`max_pages`).
- **Timeout Control**: Sets a timeout for each request to avoid hanging on slow or unresponsive sites.
- **Concurrency Limit**: Controls how many pages can be processed at the same time for better resource management.

### Data Processing
- **Content Filtering**: Articles are processed to check if they contain future dates or match user-specified keywords.
- **Deduplication**: Uses Simhash for content fingerprinting to avoid saving duplicate articles.
- **Date Detection**: Employs `dateparser` to parse various date formats mentioned in articles, focusing on future events or plans.

### Storage and Reporting
- **Article Saving**: Saves articles in JSON format, including metadata like URL and publication date.
- **Logging**: All operations are logged in JSON lines format for traceability.
- **Reports Generation**: 
  - A text report summarizing the crawl statistics.
  - An HTML report detailing articles with future content, including sentences that mention future dates.

### User Commands During Execution
- **Stop**: Manually stop the crawler.
- **Status**: Check current crawl statistics.
- **Articles**: Display a list of found articles.
- **Future**: Show articles containing future dates.
- **Search**: Search within found articles using keywords.
- **Keywords**: Add or change search keywords dynamically.
- **Days**: Adjust the publication date range on-the-fly.
- **Cities**: Change the selection of cities to crawl.

## Requirements
- Python 3.7+
- `aiohttp` for asynchronous HTTP requests
- `BeautifulSoup` for HTML parsing
- `dateparser` for parsing dates in various formats
- `jieba` for Chinese text segmentation
- `simhash` for content deduplication
- `json` for JSON operations
- `logging` for logging operations

